# ML2_fraud_detection

Scoring: roc_auc, precission-recall curve area

exeriments: https://dagshub.com/azhgh22/ML2_fraud_detection.mlflow/#/experiments/1?searchFilter=&orderByKey=attributes.start_time&orderByAsc=false&startTime=ALL&lifecycleFilter=Active&modelVersionFilter=All+Runs&datasetsFilter=W10%3D




1. Logistic Regression
    run1: 
        1. თავდაპირველად გამოვიყენე მხოლოდ ტრანზაქციების ცხრილი.
        2. random split {train:80%, val-test = 10-10%}
        3. დავდროფე ისეთი სვეტების სადაც na-ები 90% მეტი იყო. ამ კლასსის threshold- tuning-ის შედეგად შეირჩა
        4. ამ ეტაპზე missing value -ებს უბრალოდ მოდით ვავსებ.
        5. ენკოდირებისთვის ვიყენებ woe-ს
        6. სვეტებს ვასქეილებ standard-scaler-ით

        შედეგები:
            train_roc_auc 0.8574097654982666
            test__roc_auc 0.8544036065433154
            train_pr_auc 0.44952281311409265
            val_pr_auc 0.44285749232171784

        როგორც შედეგებიდან ჩანს საკმაოდ მაღალი ბაიასია. გარდა ამისა რადაგანაც დატა არ არის ბალანსირებული
        precission-recall-ზე დაკვირვებით ვიგებთ რომ დადებითი მნიშვნელობები მოდელმა ვერ ისწავლა.
        roc-auc არაბალანსირებული დატის შემთხვევაშიც კი საკმაოდ მაღალ შეფასებას წერს, რადგან დამოკიდებულია
        negative-მნიშვნელობებზე. ამიტომაც pre-recall რომელიც უფრო მეტად ყურადღებას possitive-ებს აქცევს
        უკეთესი შემფასებელია.
    
    run2:
        ამ ეტაპზე ბაიასის გამოსწორებაზე არ მიფიქრია. უფრო მეტი ყურადღება მიექცა როგორმე feature-ების რაოდენობა შემემცირებინა. ამიტომ დავამატე correlation filter და variance filter
        
        correlation filter:
            threshold -ის შერჩევის შემდეგ ტუნინგის დახმარებით კორელაციისთვის საუკეთესო აღმოჩნდა 0.98.
            ერთმანეთთან ძალიან კორერილებული ცვლადებიდან ერთ-ერთს ვაგდებ.


        variance filter: ამ შემთხვევაში პირდაპირ ვარიაციაზე დაკვირვება შედეგის მომტანი არ იქნებოდა, რადაგან
        ყველა ცვლადი სხვადასხვა შუალედშია მოთავსებული. მათი ვარიაციის მაჩვენებლებიც განსხვავებული.
        განსხვავებულში იგულისხმება, რომ კონკრეტული threshold-ის აღება შეიძლებელია, რომელიც მასზე მეტი ან
        ნაკლები ვარიაციის მქონე სვეტებს გადაყრის. 
            ამიტომ გამოვიყენე ვარიაციის შემდეგი შემფასებელი. ამ ნაწილის მიზანია, რომ ძაალიან პატარა ვარიაციის მქონე სვეტები გადაყაროს. ვაკვირდები კონკრეტულ სვეტში ყველაზე ხშირ მნიშვნელობას. თუ მისი სიხშირე
            მეტია არა Nan მნიშვნელობის მქონე სვეტების რაოდენობის რაღაც ნაწილზე(threshold-ით განისაზღვრება)
            მაშინ ამ სვეტს ვყრი
            
        cross validation Tunning-ის შედეგად გაირკვა რომ 0.85 საუკეთესო შედეგს დებს. უფრო მაღალი ან უფრო დაბალი
        threshold-ების შემთხვევაში score გაუარესდა.

        train_roc_auc 0.8305824495246193
        test__roc_auc 0.8344317929035135
        train_pr_auc 0.36696648914532415
        val_pr_auc 0.35982104429199857 

        ბაიასი არ შემცირებულა. score გაუარესდა.

    run3: როგორც უკვე ვთქვი დატა ძალიან არაბალნსირებულია. ეს კი იწვევს იმას რომ ტრენინგის დროს 
         მოდელი ვერ აქცევს ყურადღებას დადებით მნიშვნელობებს, და სწავლობს მხოლოდ უარყოფითებს.
         ამიტომ ამ run-ში გამოვიყენე undersampling - რომელიც უარყოფითების რაოდენობას შემთხვევითი შერჩევით
         ამცირებს ისე რომ დადებითები მათი 10% გახდეს.

        train_roc_auc 0.8411548386186528
        test__roc_auc  0.8460401620053756
        train_pr_auc 0.3814995798917209
        val_pr_auc 0.37712108928081595

        შედეგებიდან ჩანს რომ მოდელმა შედარებით უკეთ დაისწავლა დადებითები რაც score-ის გაუმჯობესებაში გამოიხატება, თუმცა bias არ შემცირებულა

    run4:  ამ ეტაპზე transaction და identity ცხრილი ერთმანეთთან დავმერჯე. და როგორც ჩანს შედეგებიდან 
        გამოჩნდება identity ცხრილი მნიშვნელოვანი ინფორმაციის მატარებელია რადგან score გაიზარდა.

        train_roc_auc 0.907853302874848
        test__roc_auc 0.9049321735116206
        train_pr_auc 0.6717528483231122
        val_pr_auc 0.6627410879062077

    
    თუნცა საბოლოო ჯამში იმის თქმა შეიძლება, რომ Logistc Regression არ არის საკმარისად კომპლექსური მოდელი,
    ამ დატის აღსაწერად, რაც დიდ bias -ში გამოიხატება.

2.  XGboost

    run_1: 
        ჯერჯერობით ყველაფერი იგივე უბრალოდ logistic-ის ნაცლად xgboost-ის ვიყენებ.

        როგორც წინა მოდელიდან გამოჩნდა გვჭირდება უფრო კომპლექსური მოდელი.
            XGboost bias შეაცირა თუმცა ვარიაცია გაზარდა. 

        train_roc_auc 0.9614761328835242
        test__roc_auc 0.9389797364926739
        train_pr_auc 0.7683745191152509
        val_pr_auc 0.6869547529926503

        გარდა ამისა preccision-recall -ზე დაკვირვებით ჩანს რომ მართალია წინა მოდელთან შედარებით შედეგი
        გაუმჯობესდა, მაგრამ მოდელს მაინც უჭირს განცალკევება.


    run_2: ამ ნაწილში რამდენიმე რამ გაუმჯობედა.
        1. წინა run-ზე დაკვირვებით აღმოჩნდა რომ identity სვეტის უსარგებლო სვეტები არ იყრებოდა კორელაციისა, 
            და ვარიაციის ფილტრის 
        
            ამიტომ დატაზე დაკვირვებით შევარჩიე დაბალი ვარიაციის მქონე სვეტები და ხელით გადავყარე.

        2. დატა არის დროზე დამოკიდებული. ამიტომ radnom split -ის ნაცვლად ახლა დასორტე 'TransactionDT'
            ის მიხედვით და ისე გავყავი.

        3. მართალია undersampling-ს ვაკეთებთ, თუნცა იმის გამო რომ ბევრი row არ დაკარგულიყო, ასევე დავამატე
            oversampling. 
            ანუ ჯერ undersampling -ით 10% ფარდობაზე დამყავს.
            ხოლო შემდეგ oversampling-ით 30%-ზე. 

            თუმცა საშიშროება ის არის რომ, ზედმეტი ბალანსირებით შეიძლება დატა დამახინჯდეს რაც bias გაზრდის.

        ამ შემთხვევაში შედეგი გაუარესდა :)
        
        
        train_roc_auc 0.9997942452771864
        test__roc_auc 0.9101152413885607
        train_pr_auc 0.9922815912808441
        val_pr_auc 0.5612802902035484

        როგორც ვხედავთ დიდი ვარიაცია გვაქვს. train დაიზეპირა.

    run4: ვარიაციის შესამცირებლად xgboos-ს max-depth და რეგულარიზაცია დავამატე.
        cross-validation-ის შედეგად 
            max_depth 40
            reg_lambda 10

            საუკეთსო შედეგი აჩვენა.

        მართალია ვარიაცე შემცირდა, მაგრამ მაინც ძალიან დიდია.

        train_roc_auc 0.9998985665011281
        test__roc_auc 0.923217867126044
        train_pr_auc 0.9964334513981618
        val_pr_auc 0.5803687242402324

    run5: ამ  run-ში კატეგორიულების გადაყანის პრონციპი შეიცვალა.
         ისეთ კატეგორიულებს რომლებას 5 ზე ნაკები მნიშვნელობა აქვთ one-hot -ით გადაგვყავს.
         დანარჩენები ისევ woe encoding-ით

        5-იანი ისე cross valiation-ით შერჩეული რიცხვია, საუკეთესო შედეგი მან აჩვენა

        train_roc_auc 0.9999999890485821
        test__roc_auc 0.9200173246465221
        train_pr_auc 0.9999996994779938
        val_pr_auc 0.6117097772025055

        კვლავ დიდი ვარიაციაა, თუმცა score გაუმჯობესდა

    run6: ამ ეტაპზე feature-ების რაოდენობის შესამცირებლად დავამატე LogisticFilter
        სანამ xgboost-ზე დატრენინგდება, ვატრენინგებ logisticregression-ზე და ვფილტრავ მინიჭებული წონების
        მოდულის მიხედვით. დაბალ წონიანებს ვაგდებ.

        თუმცა შედეგი გააუარესა.
        train_pr_auc 0.993568520640787
        val_pr_auc 0.5133231577016472

        ვარიაცია კიდევ უფრო გაზარდა. როგორც ჩანს მნიშვნელოვანი feature-ი გადააგდო და target-თან ნაკლებად
        კორერირებული დატოვა.


    run7: ყველა feature დავტოვე და დავატრენინგე XGboost: n_estimators = 5000
        თან ვიყენებ მხოლოდ ტრანზაქციებს. 

            train_roc_auc 0.9999999989316463
            test__roc_auc 0.9631032152383314
            train_pr_auc 0.9999999703345377
            val_pr_auc 0.8267291973402419

        ვარიაცია მნიშვნელოვნად შემცირდა. ეს იმიტომ რომ როგორც ჩანს წინა run-ებში ფილტრების დახმარებით
        მნიშვნელოვან ინფორმაციას ვკარგავდი.


    run8: ეს ნაწილი run7 -ის განახლებაა.
        კატეგორიულების გადაყვანისას ვიყენებ TargetEnocder.
        და ვარიაციის კიდევ უფრო შესმცირებლად დავატუნინგე max_depth =40, reg_lamda=250 ჰიპერპარამეტრები

    

3. Decision Tree
    run1: აქ დავატრენინგე გადაწყვეტილებათა ხე, თავიდან როგორც მოსალოდენლი იყო ჰქონდა დიდი ვარიაცია.
        თუმცა ჰიპერპარამეტრების ტუნინგის შედეგად: {
            max_depth 20
            min_samples_split 60
        }

        ვარიაცია შემცირდა. თუმცა score-იც გაუარესდა.


        train_roc_auc 0.9010704652019186
        test__roc_auc 0.8598904581893422
        train_pr_auc 0.6325186699010714
        val_pr_auc 0.5216229173900133

    run2: წინა რანთან შედარებით აქ ვცდილობ დატის დაბალანსებას. undersampling-სა და oversampling-ის გამოყენებით.
            თუმცა კვლავ დიდი ვარიაციაა.


4.  RandomForrest
    მიზანია Bagging-ის გზით ვარიაციის შემცირება.

    აქ კვლავ ვიყენებ კორელაცის, ვარიაციის ზემოხსენებულ მეთოდებს. 

    RandomForest-ის ჰიპერპარამეტრების cross validation-ის გზით არჩევისას. 
    საუკეთესო შედეგი ამაც აჩვენა: {
        n_estimatros: 100
        max_depth: 40
        min_samples_split :20
    }

    train_roc_auc 0.9925165617172038
    test__roc_auc 0.9323579924789215
    train_pr_auc 0.9131510652457692
    val_pr_auc 0.6800089539807297

    ვარიაცია შემცირდა.


5. უკვე განვლილი მეთოდებიდან ყველაზე კარგი შედეგი მაინც xgboost-ს ჰქონდა, მისი პრობლემა კი იყო ვარიაცია.
    ამ ნაწილში ვატრენინგებ bagingClassifier-ს ოღონდ estimator-ად ვიყყენებ xgboost-ს

    ჰიპერპარამეტრების ტუნინგის შემდეგ ვარიაცია მართლაც შემცირდა, და თან score-იც არ გაუარესებულა.

        train_roc_auc 0.9938808749258385
        test__roc_auc 0.9542880973743413
        train_pr_auc 0.930560450269025
        val_pr_auc 0.7733469144467339

საბოლოო ჯამში ავირჩიე bagging+xgboost მეთოდი საუკეთესო მოდელად, რადაგან score დაახლოებით იგივე ჰქონდა თუნცა, 
ამ უკანასკნელს ვარიაცია უფრო დაბალი იყო

score on kaggle : 0.912684